# -*- coding: utf-8 -*-
"""Copy of text_mining_project_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vHVoXOWynp6WcNU1gUN_HjkhlJuhDP4t

# Import libs
"""


import pandas as pd
import numpy as np
import os

from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, AutoModel
from datasets import Dataset
from datasets import load_metric
import re 
from underthesea import word_tokenize


"""# Hyperparameters"""

# setup dataset & tokenizer
num_labels = 6
max_seq_len = 256
overwrite_output_dir = True

# Training 
epochs = 3
train_batch_size = 16
val_batch_size = 16
lr = 1e-5
logging_steps = 10

step_per_epoch = int(len(train_df) / train_batch_size)
warmup_steps = int(step_per_epoch * 2/3)    #takes 2/3 first epoch for warming up

weight_decay = 0.28
gradient_accumulation_steps = 3
eval_steps = 50
# output_dir = './results/'
# logging_dir = './logs/'

"""# Load Model + tokenizer"""

tokenizer = AutoTokenizer.from_pretrained("vinai/phobert-base")
#model = AutoModelForSequenceClassification.from_pretrained("vinai/phobert-base", num_labels= num_labels)



def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True, max_length = max_seq_len)


model_load = AutoModelForSequenceClassification.from_pretrained("pretrained_model/final_model_6C")

"""## Trainer for model"""

# arguments for Trainer
inf_batch_size = 64

test_args = TrainingArguments(
    output_dir = "tmp_trainer",
    do_train = False,
    do_predict = True,
    per_device_eval_batch_size = inf_batch_size    
)

trainer = Trainer(
    model=model_load, 
    args = test_args                       
)

"""## Predict"""

from scipy.special import softmax

def preprocess(txt):
    def clean_html(raw_html):
        CLEANR = re.compile('<.*?>')    
        cleantext = re.sub(CLEANR, '', raw_html)
        return cleantext

    def remove_special_chars(txt):
        regex = r"[^,.!\"\'(...);\w\s]"
        return re.sub(regex, '', txt)

    def normalize_unicode(txt):
        def loaddicchar():
            dic = {}
            char1252 = 'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'.split(
                '|')
            charutf8 = "à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ".split(
                '|')
            for i in range(len(char1252)):
                dic[char1252[i]] = charutf8[i]
            return dic
        
        
        dicchar = loaddicchar()
        

        return re.sub(r'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ',
                    lambda x: dicchar[x.group()], txt)

    def tokenize(txt):
        return word_tokenize(txt, format="text")

    temp = txt
    temp = clean_html(temp)
    temp = remove_special_chars(temp)
    temp = normalize_unicode(temp)
    temp = temp.strip()

    temp = tokenize(temp)
    return temp

def predict(trainer, text):
    def create_dataset_inference(text):
        df = pd.DataFrame({'text': text})
        df['text'] = df['text'].astype(str)
        df['text'] = df['text'].apply(preprocess)

        dataset = {'text': df['text']}
        dataset = Dataset.from_dict(dataset)
        return dataset

    dataset = create_dataset_inference(text)
    dataset = dataset.map(tokenize_function, batched=True)
    # predict
    preds = np.array(trainer.predict(dataset)[0])

    # convert to label
    label = np.argmax(preds, axis = 1)
    prob = np.max(softmax(preds, axis = 1), axis = 1)
    
    pred_df = pd.DataFrame({'text': text, 'label': label, 'probability': prob})
    pred_df['label'] = pred_df['label'].replace([0,1,2,3,4,5], ['bao chi','sinh hoat','nghe thuat', 'khoa hoc', 'hanh chinh','chính luận'])
    return pred_df


